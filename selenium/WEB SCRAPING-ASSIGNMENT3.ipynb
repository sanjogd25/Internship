{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1936d2cf",
   "metadata": {},
   "source": [
    "1. a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required Library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27481322",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b155f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking the user to input the keywords he/she wants to search\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c72799",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar=driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(user_inp)\n",
    "time.sleep(3)\n",
    "search_btn=driver.find_element_by_id(\"nav-search-submit-button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9b4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843f67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40ee5db",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your  search results and save it in a data frame and csv. In case if any product has less than 3 pages in search  results then scrape all the products available under that product name. Details to be scraped are: \"Brand  Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and  “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b76b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Guitar product we are scraping all required detail.\n",
    "\n",
    "product_url = [] #creating empty list file to collect each product URL\n",
    "\n",
    "\n",
    "# using for loop collecting all 3 pages all product Url\n",
    "for i in range(0,2):  # next 2 pages moved by this loop\n",
    "    time.sleep(2)\n",
    "  \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-no-outline']\"):\n",
    "        product_url.append(j.get_attribute(\"href\"))\n",
    "\n",
    "    \n",
    "# creating the empty list to scrap the data\n",
    "brand_name = []\n",
    "product_name = []\n",
    "rating = []\n",
    "num_rating = []\n",
    "price = []\n",
    "return_exchange = []\n",
    "exp_delivery = []\n",
    "avilablity = []\n",
    "other_detail = []\n",
    "\n",
    "\n",
    "# Scraping the all detail of each product\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    # Scraping product name\n",
    "    try:\n",
    "        product = driver.find_element_by_id(\"productTitle\")\n",
    "        product_name.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        product_name.append(\"-\")\n",
    "         \n",
    "    #Scraping brand name\n",
    "    try:\n",
    "        brand= driver.find_element_by_id(\"bylineInfo\")\n",
    "        spl=brand.text.replace(\"Visit the \",\"\")\n",
    "        spl=spl.replace(\"Store\",\"\")\n",
    "        spl=spl.replace(\"Brand: \",\"\")\n",
    "        brand_name.append(spl)\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append(\"-\")\n",
    "  \n",
    "    \n",
    "    #Scraping rating\n",
    "    try:\n",
    "        rate = driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']//span\")\n",
    "        rating.append(rate.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append(\"-\")\n",
    "    \n",
    "    #Scraping price of the product\n",
    "    try:\n",
    "        pri = driver.find_element_by_xpath(\"//span[@class='a-price aok-align-center']\")\n",
    "        price.append(pri.text.replace(\"\\n00\",\"\"))\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"-\")\n",
    "     #Scraping return/exchange of the product    \n",
    "    try:\n",
    "        return_pol =driver.find_element_by_xpath(\"//span[@class='a-declarative']//div[2]//a\")\n",
    "        return_exchange.append(return_pol.text)\n",
    "    except NoSuchElementException:\n",
    "        return_exchange.append(\"-\")\n",
    "    #Scraping delivery of the product            \n",
    "    try:            \n",
    "        exp_del = driver.find_element_by_xpath(\"//div[@id='ddmDeliveryMessage']//b\")\n",
    "        exp_delivery.append(exp_del.text)\n",
    "    except NoSuchElementException:\n",
    "         exp_delivery.append(\"-\") \n",
    "            \n",
    "    #Scraping availability of the product \n",
    "    try:\n",
    "        avil = driver.find_element_by_xpath(\"//div[@id='availability']//span\")\n",
    "        avilablity.append(avil.text)\n",
    "    except NoSuchElementException:\n",
    "        avilablity.append(\"-\")\n",
    "   \n",
    "                 \n",
    "    time.sleep(1)\n",
    "\n",
    "# Creating Dataframe \n",
    "Guitar = pd.DataFrame()\n",
    "Guitar[\"Brand\"] = brand_name\n",
    "Guitar[\"Name of the Product\"] = product_name\n",
    "Guitar[\"Rating\"] = rating\n",
    "Guitar[\"Price\"] = price\n",
    "Guitar[\"Return/Exchange\"] = return_exchange\n",
    "Guitar[\"Expected Delivery\"] = exp_delivery\n",
    "Guitar[\"Availability\"] = avilablity\n",
    "Guitar[\"Product URL\"] = product_url \n",
    "\n",
    "Guitar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547dfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c6102d2",
   "metadata": {},
   "source": [
    "3. a python program to access the search bar and search button on images.google.com and scrape 10images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98351d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening images.google.com\n",
    "driver.get('https://images.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function which takes the necessary input as argument\n",
    "drive = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# empty list creating to scrap the detail\n",
    "fruit_image = []\n",
    "car_image = []\n",
    "machineLearning_image = []\n",
    "guitar_image = []\n",
    "cakes_image = []\n",
    "\n",
    "\n",
    "# iterating 3 types of image\n",
    "for i in ['fruits','cars','machine learning','Guitar','cakes']:\n",
    "    # opening google img website\n",
    "    url = \"https://images.google.com/?gws_rd=ssl\"\n",
    "    driver.get(url)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    \n",
    "    #Scraping the fruits detail\n",
    "    if i == 'fruits':\n",
    "        # Accessing search bar and search button\n",
    "        search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\").send_keys(i)\n",
    "        search_btn=driver.find_element_by_xpath(\"//span[@class= 'z1asCe MZy1Rb']\")\n",
    "        driver.execute_script('arguments[0].click()', search_btn)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # first 10 image of fruit scraping\n",
    "        for i in range(0,3):\n",
    "            for i in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n",
    "                if len(fruit_image) < 10:\n",
    "                    fruit_image.append(i.get_attribute(\"src\"))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000)\")#scroll down the website\n",
    "            time.sleep(4)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Scraping the cars detail    \n",
    "    elif i == 'cars':\n",
    "        # Accessing search bar and search button\n",
    "        search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\").send_keys(i)\n",
    "        time.sleep(2)\n",
    "        search_btn=driver.find_element_by_xpath(\"//span[@class= 'z1asCe MZy1Rb']\")\n",
    "        driver.execute_script('arguments[0].click()', search_btn)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # first 10 image of car scraping\n",
    "        for i in range(0,3):\n",
    "            for i in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n",
    "                if len(car_image) < 10:\n",
    "                    car_image.append(i.get_attribute(\"src\"))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000)\") #scroll down the website\n",
    "            time.sleep(4)\n",
    "        \n",
    "    #Scraping the machine learning detail     \n",
    "    elif i == 'machine learning':\n",
    "        # Accessing search bar and search button\n",
    "        search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\").send_keys(i)\n",
    "        time.sleep(2)\n",
    "        search_btn=driver.find_element_by_xpath(\"//span[@class= 'z1asCe MZy1Rb']\")\n",
    "        driver.execute_script('arguments[0].click()', search_btn)        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # first 10 image of machinelearning scraping        \n",
    "        for i in range(0,3):\n",
    "            for i in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n",
    "                if len(machineLearning_image) < 10:\n",
    "                    machineLearning_image.append(i.get_attribute(\"src\"))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000)\")#scroll down the website\n",
    "            time.sleep(4) \n",
    "\n",
    "   #Scraping the guitar detail\n",
    "    if i == 'Guitar':\n",
    "        # Accessing search bar and search button\n",
    "        search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\").send_keys(i)\n",
    "        time.sleep(2)\n",
    "        search_btn=driver.find_element_by_xpath(\"//span[@class= 'z1asCe MZy1Rb']\")\n",
    "        driver.execute_script('arguments[0].click()', search_btn)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # first 10 image of fruit scraping\n",
    "        for i in range(0,3):\n",
    "            for i in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n",
    "                if len(guitar_image) < 10:\n",
    "                    guitar_image.append(i.get_attribute(\"src\"))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000)\")#scroll down the website\n",
    "            time.sleep(4)\n",
    "        \n",
    "   #Scraping the cakes detail\n",
    "    if i == 'cakes':\n",
    "        # Accessing search bar and search button\n",
    "        search_bar = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\").send_keys(i)\n",
    "        time.sleep(2)\n",
    "        search_btn=driver.find_element_by_xpath(\"//span[@class= 'z1asCe MZy1Rb']\")\n",
    "        driver.execute_script('arguments[0].click()', search_btn)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # first 10 image of fruit scraping\n",
    "        for i in range(0,3):\n",
    "            for i in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n",
    "                if len(cakes_image) < 10:\n",
    "                    cakes_image.append(i.get_attribute(\"src\"))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            driver.execute_script(\"window.scrollTo(0, 1000)\")#scroll down the website\n",
    "            time.sleep(4)\n",
    "                            \n",
    "            \n",
    "            \n",
    "#creating dataframe            \n",
    "google = pd.DataFrame()\n",
    "google['Fruits'] = fruit_image  \n",
    "google['Cars'] = car_image\n",
    "google['Machine Learning'] = machineLearning_image\n",
    "google['Guitar'] = guitar_image\n",
    "google['Cakes'] = cakes_image\n",
    "\n",
    "google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ac829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "482d8125",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943acdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the chrome driver\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Accessing link of flipkart to search smart phone \n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "#Seaching mobile\n",
    "search_bar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "#User input\n",
    "text = input(\"Enter the product :\")\n",
    "search_bar.send_keys(text)\n",
    "time.sleep(2)\n",
    "#Searching the product\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "driver.execute_script('arguments[0].click()', search_btn)\n",
    "time.sleep(3)\n",
    "\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "\n",
    "#details need to scrap\n",
    "brand_name = []\n",
    "smart_phone_name = []\n",
    "color = []\n",
    "ram = []\n",
    "storage = []\n",
    "pri_cam = []\n",
    "sec_cam =[]\n",
    "bat_capacity = []\n",
    "price = []\n",
    "product_url =[]\n",
    "dis_size = []\n",
    "dis_res = []\n",
    "\n",
    "\n",
    "#scraping\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")[:24]:\n",
    "        brand_name.append(i.text.split(\" \")[0])\n",
    "        smart_phone_name.append(i.text.split(\"(\")[0])\n",
    "        color.append(i.text.split(\"(\")[1].split(\",\")[0])\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[1]\")[:24]:\n",
    "    ram.append(i.text.split(\"|\")[0])\n",
    "    storage.append(i.text.split(\"|\")[1])   \n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[3]\")[:24]:\n",
    "    pri_cam.append(i.text.split(\"|\")[0])\n",
    "    try:\n",
    "        sec_cam.append(i.text.split(\"|\")[1])\n",
    "    except:\n",
    "        sec_cam.append(\"-\")\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[4]\")[:24]:\n",
    "    bat_capacity.append(i.text.split(\" \")[:2])\n",
    "\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_30jeq3 _1_WHN1']\")[:24]:\n",
    "    price.append(i.text)\n",
    "    \n",
    "time.sleep(2)  \n",
    "# scraping all product url \n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")[:24]:\n",
    "    product_url.append(i.get_attribute(\"href\"))\n",
    "    time.sleep(3) \n",
    "    \n",
    "# display resolution and size scraping \n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "        \n",
    "    # handling the exception\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, 2000)\")\n",
    "        driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\").click()\n",
    "        time.sleep(4)\n",
    "    except ElementClickInterceptedException:\n",
    "        pass\n",
    "        time.sleep(2)\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, 2100)\")\n",
    "        time.sleep(4)\n",
    "        d_siz = driver.find_element_by_xpath(\"//li[2][@class='_21Ahn-']\")\n",
    "        dis_size.append(d_siz.text)\n",
    "    except:\n",
    "        dis_size.append(\"-\")\n",
    "\n",
    "            \n",
    "# scraping all product url            \n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, 2100)\")\n",
    "        driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\").click()\n",
    "        time.sleep(4)\n",
    "    except ElementClickInterceptedException:\n",
    "        pass\n",
    "    \n",
    "      \n",
    "\n",
    "#Creating dataframe and CSV file\n",
    "SmartPhone = pd.DataFrame()\n",
    "SmartPhone['Brand'] = brand_name\n",
    "SmartPhone['Smart Phone Name']= smart_phone_name\n",
    "SmartPhone['Color']=color\n",
    "SmartPhone['Ram']= ram\n",
    "SmartPhone ['Stroage(ROM)'] = storage \n",
    "SmartPhone ['Primary Camera'] = pri_cam\n",
    "SmartPhone ['Secondary Camera'] = sec_cam\n",
    "SmartPhone ['Display Size']=dis_size\n",
    "SmartPhone ['Battery Capacity'] = bat_capacity\n",
    "SmartPhone ['Price'] = price\n",
    "SmartPhone ['Product link'] = product_url\n",
    "\n",
    "SmartPhone.to_csv(\"SmartPhone.csv\")\n",
    "SmartPhone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19656b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09096d3",
   "metadata": {},
   "source": [
    "5. a program to scrap geospatial coordinates (latitude, longitude) of a city searched on googlemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\chromedriver.exe\")\n",
    "driver.get('https://www.google.com/maps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to enter the city name \n",
    "city = input('Enter City Name : ') \n",
    "driver.find_element_by_id('searchboxinput').send_keys(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e19c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the entered city latitude and Longitude from google map\n",
    "lat = driver.current_url.split('/@')[1].split(',')[0]\n",
    "long = driver.current_url.split('/@')[1].split(',')[1].split(',')[0]\n",
    "print('Latitude of entered city is :', lat, '\\nLongitude of entered city is :', long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6097c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eedf7de3",
   "metadata": {},
   "source": [
    "6. a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81364802",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://trak.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e100211",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for i in driver.find_elements_by_xpath('//td[@class = \"column-2\"]')[:29]:\n",
    "    date.append(i.text.replace('/n',''))\n",
    "    \n",
    "#performed indexing for each column to fetch the data between January 21 – March 21\n",
    "date = date[5:]\n",
    "startup_name = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-3\"]')[:29]:\n",
    "    startup_name.append(j.text.replace('/n',''))\n",
    "startup_name = startup_name[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-4\"]')[:29]:\n",
    "    industry.append(j.text.replace('/n',''))\n",
    "industry = industry[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "subvertical = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-5\"]')[:29]:\n",
    "    subvertical.append(j.text.replace('/n',''))\n",
    "subvertical = subvertical[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class = \"column-6\"]')[:29]:\n",
    "    loc.append(j.text.replace('/n',''))\n",
    "loc = loc[5:]\n",
    "\n",
    "investor_name = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-7\"]')[:29]:\n",
    "    investor_name.append(j.text.replace('/n',''))\n",
    "investor_name = investor_name[5:]\n",
    "\n",
    "investment_type = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-8\"]')[:29]:\n",
    "    investment_type.append(j.text.replace('/n',''))\n",
    "investment_type = investment_type[5:]\n",
    "\n",
    "amt = []\n",
    "for j in driver.find_elements_by_xpath('//td[@class=\"column-9\"]')[:29]:\n",
    "    amt.append(j.text.replace('/n',''))\n",
    "amt = amt[5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03435f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = pd.DataFrame({})\n",
    "funding_df['Date'] = date\n",
    "funding_df['Startup name'] = startup_name\n",
    "funding_df['Industry'] = industry\n",
    "funding_df['Subvertical'] = subvertical\n",
    "funding_df['Location'] = loc\n",
    "funding_df['Investor name'] = investor_name\n",
    "funding_df['Investment tpye'] = investment_type\n",
    "funding_df['Amount(in USD)'] = amt\n",
    "\n",
    "funding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df.to_csv(\"Indian Startups_Q2_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005d1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967942c6",
   "metadata": {},
   "source": [
    "7. a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "#Q7.webpage\n",
    "url = 'https://www.digit.in/'  \n",
    "driver.get(url)\n",
    "#click search button\n",
    "Lap=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[4]/ul/li[3]/a\")\n",
    "driver.execute_script('arguments[0].click()', Lap)\n",
    "time.sleep(2)\n",
    "\n",
    "G_laptops=driver.find_element_by_xpath(\"/html/body/div[6]/div/div[2]/div[2]/ul/li[10]/a\")\n",
    "driver.execute_script('arguments[0].click()', G_laptops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Empty list\n",
    "name = []\n",
    "Price = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "RAM = []\n",
    "weight = []\n",
    "dimension = []\n",
    "GPU = []\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping names\n",
    "names=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    name.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping operating system\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping display\n",
    "displays=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    display.append(i.text)\n",
    "\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping memory\n",
    "memories=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")# extrat HDD and RAM form xpath\n",
    "for i in memories:\n",
    "    HDD.append(i.text.split(\"/\")[0])\n",
    "    \n",
    "    RAM.append(i.text.split(\"/\")[1])\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping weight\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")# extrat weight form xpath\n",
    "for i in weights:\n",
    "    weight.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping dimension\n",
    "dimension=[]\n",
    "dimensions=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\") \n",
    "for i in dimensions:\n",
    "    dimension.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping graphical processor\n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\") \n",
    "for i in GPUs:\n",
    "    GPU.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#scraping price\n",
    "price=driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[3]\")\n",
    "for i in price:\n",
    "    Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf670e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make data frame\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"price\":Price,\n",
    "                \"OS\":OS,\n",
    "                \"Display\":display,\n",
    "                \"HDD\":HDD,\n",
    "                \"RAM\":RAM,\n",
    "                \"Weight\":weight,\n",
    "                \"Dimension\":dimension,\n",
    "                \"Graphical processor\":GPU})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7844b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a61c6f4d",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90844bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "#Q8.webpage\n",
    "url = 'https://www.forbes.com/?sh=69e6b8c92254'  \n",
    "driver.get(url)\n",
    "\n",
    "#click search button\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "search_btn.click()\n",
    "\n",
    "#click search button\n",
    "search_btn = driver.find_element_by_xpath(\"//div[@class='header__channels--wrapper']\")\n",
    "search_btn.click()\n",
    "\n",
    "#select billionaire  \n",
    "\n",
    "billioners = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "\n",
    "billioners.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "#select world billionaire  \n",
    "\n",
    "world_billioners= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "\n",
    "world_billioners.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c54161",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = []\n",
    "name = []\n",
    "networth = []\n",
    "age = []\n",
    "citizenship = []\n",
    "source = [] \n",
    "industry = []\n",
    "\n",
    "for i in range(0,15):\n",
    "    nm = driver.find_elements_by_xpath('//div[@class=\"personName\"]')\n",
    "    rnk = driver.find_elements_by_xpath('//div[@class=\"rank\"]')\n",
    "    worth = driver.find_elements_by_xpath('//div[@class=\"netWorth\"]')\n",
    "    ag = driver.find_elements_by_xpath('//div[@class=\"age\"]')\n",
    "    citi = driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]')\n",
    "    sour = driver.find_elements_by_xpath('//div[@class=\"source-column\"]')\n",
    "    indus = driver.find_elements_by_xpath('//div[@class=\"category\"]')\n",
    "    \n",
    "    for i in rnk:\n",
    "        rank.append(i.text.replace('.',''))\n",
    "    for j in nm:\n",
    "        name.append(j.text)\n",
    "    \n",
    "    for k in worth:\n",
    "        networth.append(k.text)\n",
    "        \n",
    "    for l in ag:\n",
    "        age.append(l.text)\n",
    "        \n",
    "    for m in citi:\n",
    "        citizenship.append(m.text)\n",
    "        \n",
    "    for n in sour:\n",
    "        source.append(n.text)\n",
    "    \n",
    "    for o in indus:\n",
    "        industry.append(o.text)\n",
    "        \n",
    "    \n",
    "        \n",
    "    try:\n",
    "        nxt = driver.find_element_by_xpath('//button[@class=\"pagination-btn pagination-btn--next \"]')\n",
    "        nxt.click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "billionaires = pd.DataFrame({})\n",
    "billionaires['Rank'] = rank\n",
    "billionaires['Name'] = name\n",
    "billionaires['NetWorth'] = networth\n",
    "billionaires['Age'] = age\n",
    "billionaires['Citizenship']  = citizenship\n",
    "billionaires['Source'] =  source\n",
    "billionaires['Industry'] = industry\n",
    "billionaires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955fc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd1c2962",
   "metadata": {},
   "source": [
    "9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get(\"https://www.youtube.com/watch?v=kffacxfA7G4\")\n",
    "\n",
    "# To scroll down\n",
    "for _ in range(1000):\n",
    "    driver.execute_script('window.scrollBy(0,5000)')\n",
    "    \n",
    "comment = []\n",
    "vote = []\n",
    "time = []\n",
    "\n",
    "# Scrapping Comment\n",
    "try:\n",
    "    comment_tag = driver.find_elements_by_id('content-text')\n",
    "    for i in comment_tag:\n",
    "        comment.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        comment.append(\"-\")    \n",
    "\n",
    "# Scrapping upvote\n",
    "try:\n",
    "    vote_tag = driver.find_elements_by_id(\"vote-count-middle\")\n",
    "    for j in vote_tag:\n",
    "        vote.append(j.text)\n",
    "except NoSuchElementException:\n",
    "        vote.append(\"-\")\n",
    "\n",
    "#Scrapping time\n",
    "try:   \n",
    "    time_tag = driver.find_elements_by_xpath(\"//a[@class='yt-simple-endpoint style-scope yt-formatted-string']\")\n",
    "    for k in time_tag:\n",
    "        time.append(k.text)\n",
    "except NoSuchElementException:\n",
    "        time.append(\"-\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297ef16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating DataFrame\n",
    "df = pd.DataFrame({'Comments':comment[0:500], 'Upvote':vote[0:500], 'Time':time[17:517]})\n",
    "df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62057318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e9f02b9",
   "metadata": {},
   "source": [
    "10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get(\"https://www.hostelworld.com/s?q=London, England&country=England&city=London&type=city&id=3&from=2022-05-25&to=2022-05-28&guests=2&page=1\")\n",
    "\n",
    "title = []\n",
    "distance = []\n",
    "rating = []\n",
    "total_review = []\n",
    "prices = []\n",
    "private_price = []\n",
    "dorms_price = []\n",
    "facilities=[]\n",
    "prop_details=[]\n",
    "\n",
    "title_tags = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "for i in title_tags:\n",
    "    title.append(i.text)\n",
    "    \n",
    "distance_tags = driver.find_elements_by_xpath(\"//span[@class='description']\")\n",
    "for j in distance_tags:\n",
    "    distance.append(j.text.split('-')[1])\n",
    "\n",
    "rating_tags = driver.find_elements_by_xpath(\"//div[@class='score orange big']\")\n",
    "for k in rating_tags:\n",
    "    rating.append(k.text)\n",
    "\n",
    "total_review_tags = driver.find_elements_by_xpath(\"//div[@class='reviews']\")\n",
    "for l in total_review_tags:\n",
    "    total_review.append(l.text)\n",
    "\n",
    "    \n",
    "P_prices_tags=driver.find_elements_by_xpath(\"//div[1][@class='price-col']\") #locating web element of price\n",
    "    \n",
    "for n in P_prices_tags:\n",
    "    private_price.append(n.text)\n",
    "\n",
    "\n",
    "D_prices_tags=driver.find_elements_by_xpath(\"//div[2][@class='price-col']\") #locating web element of price\n",
    "  \n",
    "for o in D_prices_tags:\n",
    "    dorms_price.append(o.text)\n",
    "\n",
    "faci_tags=driver.find_elements_by_xpath(\"//div[@class='facilities-label facilities']\")    \n",
    "for p in faci_tags:\n",
    "    facilities.append(p.text)\n",
    "\n",
    "prop_des=driver.find_elements_by_xpath(\"//div[@class='title-row']\")    \n",
    "for q in prop_des:\n",
    "    prop_details.append(q.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Hostel Name':title[:5],'Distance':distance[:5],'Rating':rating[:5],'Total Review':total_review[:5],'Private Price':private_price[:5], 'Dorms Price':dorms_price[:5],'Property Details':prop_details[:5],'Facilities':facilities[:5]})\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e0c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
